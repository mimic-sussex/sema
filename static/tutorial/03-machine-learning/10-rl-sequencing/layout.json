[
	{
		"2": {
			"drag": {
				"dragging": false,
				"top": 0,
				"left": 0
			},
			"resize": {
				"resizing": false,
				"width": 0,
				"height": 0
			},
			"responsive": {
				"valueW": 2
			},
			"min": {},
			"max": {},
			"x": 0,
			"y": 0,
			"w": 1,
			"h": 3
		},
		"3": {
			"drag": {
				"dragging": false,
				"top": 0,
				"left": 0
			},
			"resize": {
				"resizing": false,
				"width": 0,
				"height": 0
			},
			"responsive": {
				"valueW": 2
			},
			"min": {},
			"max": {},
			"x": 0,
			"y": 0,
			"w": 2,
			"h": 3
		},
		"6": {
			"drag": {
				"dragging": false,
				"top": 0,
				"left": 0
			},
			"resize": {
				"resizing": false,
				"width": 0,
				"height": 0
			},
			"responsive": {
				"valueW": 2
			},
			"min": {},
			"max": {},
			"x": 0,
			"y": 0,
			"w": 4,
			"h": 3
		},
		"8": {
			"drag": {
				"dragging": false,
				"top": 0,
				"left": 0
			},
			"resize": {
				"resizing": false,
				"width": 0,
				"height": 0
			},
			"responsive": {
				"valueW": 2
			},
			"min": {},
			"max": {},
			"x": 0,
			"y": 0,
			"w": 6,
			"h": 3
		},
		"12": {
			"drag": {
				"dragging": false,
				"top": 0,
				"left": 0
			},
			"resize": {
				"resizing": false,
				"width": 0,
				"height": 0
			},
			"responsive": {
				"valueW": 2
			},
			"min": {},
			"max": {},
			"x": 0,
			"y": 0,
			"w": 10,
			"h": 3
		},
		"id": "_8v9vdib3l",
		"data": {
			"type": "liveCodeEditor",
			"name": "hello-world_y2pqvmh6b_myyx0w13z_5ygcr56yf_6b3c4x6qe_8v9vdib3l",
			"background": "#151515",
			"lineNumbers": true,
			"hasFocus": false,
			"theme": "icecoder",
      "content": "//--------------- DON'T EDIT CODE BETWEEN THESE MARKERS\n//clock for communicating with the RL model\n:beatcount:{8}const;\n:barphasor:{1}clp;\n:clockphasor:{:beatcount:}clp;\n:clock:{:clockphasor:, [1]}rsq;\n//get the prediction from the model\n:resetseq:{:clock:,0.5}lt;\n:prediction:{{0, :clock:}fromJS, :resetseq:}mul;\n//---------------\n\t\t\t\t\t\t \n// START LIVECODING HERE!\n//master clock\n{130,4}clk;\n//mode: learning (0), predicting (1), off (2)\n:mode:{1}const;\n\t\t\t\t\t\t\t\n//this is the source sequence, the one that the model 'listens' to\t\t\t\t\t\t\t\n:source:{{4}clp, [1]}rsq;\n\t\t\t\t\t\n//this is the target sequence, that we train the model to predict\n:targetseq:{{2}clp, [2,4,2]}rsq;\n\n\n//---------------\n//choose what to play - the target or the prediction\n:seq:{[:targetseq:, :prediction:, :targetseq:], :mode:}at;\n//---------------\n\n//map the sequences to sounds\t\t\t\n:ch1:{:source:}\\auboom;\n:ch2:{:seq:,6}\\boom2;\n//add in more sounds here!\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n//final mix\n>{:ch1:, :ch2:}mix;\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\n//---------------\n//send data to the model\n:target:{[:targetseq:, {-1}const, {-1}const], :mode:}at;\n{:clock:, 0, [:source:, :target:, :barphasor:],3}toJS;\n//---------------\n\n",
      "grammarSource": "/languages/default/grammar.ne",
      "liveCodeSource": "/languages/default/code.sem",
      "grammar": "# GRAMMAR EDITOR\n\n# Lexer [or tokenizer] definition with language lexemes [or tokens]\n@{%\n\nconst lexer = moo.compile({\n  separator:      /,/,\n  paramEnd:       /}/,\n  paramBegin:     /{/,\n  listEnd:        /\\]/,\n  listBegin:      /\\[/,\n  dacoutCh:       /\\>[0-9]+/,\n  dacout:         /\\>/,\n  variable:       /:[a-zA-Z0-9]+:/,\n  sample:         { match: /\\\\[a-zA-Z0-9]+/, lineBreaks: true, value: x => x.slice(1, x.length)},\n  slice:          { match: /\\|[a-zA-Z0-9]+/, lineBreaks: true, value: x => x.slice(1, x.length)},\n  stretch:        { match: /\\@[a-zA-Z0-9]+/, lineBreaks: true, value: x => x.slice(1, x.length)},\n  clockTrig:      /0t-?(?:[0-9]|[1-9][0-9]+)(?:\\.[0-9]+)?\\b/,\n\tnumber:         /-?(?:[0-9]|[1-9][0-9]+)(?:\\.[0-9]+)?\\b/,\n  semicolon:      /;/,\n  funcName:       /[a-zA-Z][a-zA-Z0-9]*/,\n\tstring:\t\t\t\t\t{ match: /'[a-zA-Z0-9]+'/, value: x => x.slice(1,x.length-1)},\n  comment:        /\\/\\/[^\\n]*/,\n  ws:             { match: /\\s+/, lineBreaks: true},\n});\n\n%}\n\n# Pass your lexer object using the @lexer option\n@lexer lexer\n\n# Grammar definition in the Extended Backus Naur Form (EBNF)\nmain -> _ Statement _\n{% d => ( { '@lang' : d[1] } )  %}\n\nStatement ->\n  %comment _ Statement\n  {% d => d[2] %}\n\t|\n  Expression _ %semicolon _ Statement\n  {% d => [ { '@spawn': d[0] } ].concat(d[4]) %}\n  |\n  Expression _ %semicolon (_ %comment):*\n  {% d => [ { '@spawn': d[0] } ] %}\n\n\nExpression ->\n  ParameterList _ %funcName\n  {% d => sema.synth( d[2].value, d[0]['@params'] ) %}\n  |\n  ParameterList _ %sample\n  {% d => sema.synth( 'sampler', d[0]['@params'].concat( [ sema.str( d[2].value ) ] ) ) %}\n  |\n  ParameterList _ %slice\n  {% d => sema.synth( 'slice', d[0]['@params'].concat( [ sema.str( d[2].value ) ] ) ) %}\n  |\n  ParameterList _ %stretch\n  {% d => sema.synth( 'stretch', d[0]['@params'].concat( [ sema.str( d[2].value ) ] ) ) %}\n  |\n  %variable _ Expression\n  {% d => sema.setvar( d[0].value, d[2] ) %}\n  |\n  %dacout _ Expression\n  {% d => sema.synth( 'dac', [d[2]] ) %}\n  |\n  %dacoutCh _ Expression\n  {% d => sema.synth( 'dac', [d[2], sema.num(d[0].value.substr(1))] ) %}\n\nParameterList ->\n  %paramBegin Params %paramEnd\n  {% d => ( { 'paramBegin': d[0], '@params': d[1], 'paramEnd': d[2] } ) %}\n\t|\n\t%paramBegin _ %paramEnd\n  {% d => ( { 'paramBegin': d[0], '@params': [], 'paramEnd': d[2] } ) %}\n\n\nParams ->\n  ParamElement\n  {% d => ( [ d[0] ] ) %}\n  |\n  ParamElement _ %separator _ Params\n  {% d => [ d[0] ].concat(d[4]) %}\n\nParamElement ->\n  %number\n  {% d => ( { '@num': d[0] } ) %}\n\t|\n\t%string\n  {% d => ( { '@string': d[0].value } ) %}\n  |\n  Expression\n  {% id %}\n  |\n  %variable\n  {% d => sema.getvar( d[0].value ) %}\n  |\n  %listBegin Params  %listEnd\n  {% d => ( { '@list': d[1] } )%}\n\n\n# Whitespace\n\n_  -> wschar:*\n{% function(d) {return null;} %}\n\n__ -> wschar:+\n{% function(d) {return null;} %}\n\nwschar -> %ws\n{% id %}\n"
		}
	},
	{
		"2": {
			"drag": {
				"dragging": false,
				"top": 0,
				"left": 0
			},
			"resize": {
				"resizing": false,
				"width": 0,
				"height": 0
			},
			"responsive": {
				"valueW": 0
			},
			"resizable": true,
			"draggable": true,
			"min": {},
			"max": {},
			"x": 1,
			"y": 0,
			"w": 1,
			"h": 2
		},
		"3": {
			"drag": {
				"dragging": false,
				"top": 0,
				"left": 0
			},
			"resize": {
				"resizing": false,
				"width": 0,
				"height": 0
			},
			"responsive": {
				"valueW": 0
			},
			"min": {},
			"max": {},
			"x": 2,
			"y": 0,
			"w": 1,
			"h": 2
		},
		"6": {
			"drag": {
				"dragging": false,
				"top": 0,
				"left": 0
			},
			"resize": {
				"resizing": false,
				"width": 0,
				"height": 0
			},
			"responsive": {
				"valueW": 0
			},
			"min": {},
			"max": {},
			"x": 5,
			"y": 0,
			"w": 2,
			"h": 2
		},
		"8": {
			"drag": {
				"dragging": false,
				"top": 0,
				"left": 0
			},
			"resize": {
				"resizing": false,
				"width": 0,
				"height": 0
			},
			"responsive": {
				"valueW": 0
			},
			"min": {},
			"max": {},
			"x": 6,
			"y": 0,
			"w": 2,
			"h": 2
		},
		"12": {
			"drag": {
				"dragging": false,
				"top": 0,
				"left": 0
			},
			"resize": {
				"resizing": false,
				"width": 0,
				"height": 0
			},
			"responsive": {
				"valueW": 0
			},
			"min": {},
			"max": {},
			"x": 10,
			"y": 0,
			"w": 2,
			"h": 2
		},
		"id": "_524vjn8kn",
		"data": {
			"name": "hello world_au1enazhs_iwu2mp3lh_6l092dg1c_nx9yneq25_524vjn8kn",
			"id": "_524vjn8kn",
			"type": "analyser",
			"hasFocus": false,
			"theme": "monokai",
			"background": "#f0f0f0",
			"mode": "both"
		}
	},
	{
		"2": {
			"drag": {
				"top": null,
				"left": null,
				"dragging": false
			},
			"resize": {
				"width": null,
				"height": null,
				"resizing": false
			},
			"responsive": {
				"valueW": 0
			},
			"resizable": true,
			"draggable": true,
			"min": {},
			"max": {},
			"x": 1,
			"y": 2,
			"w": 1,
			"h": 2
		},
		"3": {
			"drag": {
				"top": null,
				"left": null,
				"dragging": false
			},
			"resize": {
				"width": null,
				"height": null,
				"resizing": false
			},
			"responsive": {
				"valueW": 0
			},
			"resizable": true,
			"draggable": true,
			"min": {},
			"max": {},
			"x": 2,
			"y": 2,
			"w": 1,
			"h": 2
		},
		"6": {
			"drag": {
				"top": null,
				"left": null,
				"dragging": false
			},
			"resize": {
				"width": null,
				"height": null,
				"resizing": false
			},
			"responsive": {
				"valueW": 0
			},
			"resizable": true,
			"draggable": true,
			"min": {},
			"max": {},
			"x": 5,
			"y": 2,
			"w": 2,
			"h": 2
		},
		"8": {
			"drag": {
				"top": null,
				"left": null,
				"dragging": false
			},
			"resize": {
				"width": null,
				"height": null,
				"resizing": false
			},
			"responsive": {
				"valueW": 0
			},
			"resizable": true,
			"draggable": true,
			"min": {},
			"max": {},
			"x": 6,
			"y": 2,
			"w": 2,
			"h": 2
		},
		"12": {
			"drag": {
				"top": null,
				"left": null,
				"dragging": false
			},
			"resize": {
				"width": null,
				"height": null,
				"resizing": false
			},
			"responsive": {
				"valueW": 0
			},
			"resizable": true,
			"draggable": true,
			"min": {},
			"max": {},
			"x": 10,
			"y": 2,
			"w": 2,
			"h": 2
		},
		"data": {
			"name": "hello world_1z19h81ii_zj92yxxn0",
			"type": "liveCodeParseOutput",
			"lineNumbers": false,
			"hasFocus": false,
			"theme": "shadowfox",
			"background": "rgba(25, 25, 25, 0.3)",
			"content": ""
		},
		"id": "_zj92yxxn0"
	},
	{
		"2": {
			"drag": {
				"top": null,
				"left": null,
				"dragging": false
			},
			"resize": {
				"width": null,
				"height": null,
				"resizing": false
			},
			"responsive": {
				"valueW": 0
			},
			"resizable": true,
			"draggable": true,
			"min": {},
			"max": {},
			"x": 1,
			"y": 4,
			"w": 1,
			"h": 3
		},
		"3": {
			"drag": {
				"top": null,
				"left": null,
				"dragging": false
			},
			"resize": {
				"width": null,
				"height": null,
				"resizing": false
			},
			"responsive": {
				"valueW": 0
			},
			"resizable": true,
			"draggable": true,
			"min": {},
			"max": {},
			"x": 2,
			"y": 4,
			"w": 1,
			"h": 3
		},
		"6": {
			"drag": {
				"top": null,
				"left": null,
				"dragging": false
			},
			"resize": {
				"width": null,
				"height": null,
				"resizing": false
			},
			"responsive": {
				"valueW": 0
			},
			"resizable": true,
			"draggable": true,
			"min": {},
			"max": {},
			"x": 5,
			"y": 4,
			"w": 2,
			"h": 3
		},
		"8": {
			"drag": {
				"top": null,
				"left": null,
				"dragging": false
			},
			"resize": {
				"width": null,
				"height": null,
				"resizing": false
			},
			"responsive": {
				"valueW": 0
			},
			"resizable": true,
			"draggable": true,
			"min": {},
			"max": {},
			"x": 6,
			"y": 4,
			"w": 2,
			"h": 3
		},
		"12": {
			"drag": {
				"top": null,
				"left": null,
				"dragging": false
			},
			"resize": {
				"width": null,
				"height": null,
				"resizing": false
			},
			"responsive": {
				"valueW": 0
			},
			"resizable": true,
			"draggable": true,
			"min": {},
			"max": {},
			"x": 10,
			"y": 4,
			"w": 2,
			"h": 3
		},
		"data": {
			"name": "hello world_1z19h81ii_zj92yxxn0",
			"type": "dspCode",
			"lineNumbers": false,
			"hasFocus": false,
			"theme": "shadowfox",
			"background": "rgba(25, 25, 25, 0.3)",
			"content": "// STEP 1: CONFIGURE THE VARIABLES BELOW\n// STEP 2: RUN THIS SCRIPT\n// STEP 3: START LIVECODING\n// .. and repeat\n\n// the agent keeps a history of its past inputs and corresponding actions. When training, it samples from this history.  If you want the agent to learn a wide range of behaviours, make this larger, although the agent may take longer to pick up new behaviours\nvar replayMemory = 128;\n\n//when the agent trains, this is the number of past events it learns from (randomly chosen from the replay memory)\nvar replaySize = 32;\n\n//this must match up with the :beatcount: variable in LC.  This is used to configure the model so that it knows the number of the current beat within a bar\nvar beatCount=8;\n\n//the agent must listen to the source sequence, and remember the past X events (this must be at least 1).  If the number is large, the agent may be slow to respond to changes in the source sequence\nvar sourceMemory=1;\n\n//how many of the agents own past actions will it remember and base its predictions on? (0 or above).  If the number is large, the agent may be slow to respond to changes in the source sequence.  If it's larger than sourceMemory, the agent may ignore external events in preference to its own history\nvar agentSTMSize = 0;\n\n//layer sizes of the neural network\nvar structure = [32,16,8];\n\n//display state information\nvar debug=0;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimportScripts('https://cdnjs.cloudflare.com/ajax/libs/lodash.js/4.17.15/lodash.js');\ntf.setBackend('cpu');\n\nvar createMemory = (maxMem) => {\n\tlet mem = {}\n\tmem.samples=[];\n\tmem.maxMemory= maxMem;\n\tmem.test= () => console.log(mem);\n\tmem.addSample = (sample) => {\n\t\t\tmem.samples.push(sample);\n\t\t\tif (mem.samples.length > mem.maxMemory) {mem.samples.shift()}\n\t\t};\n\tmem.sample = (n) => {\n\t\t\treturn _.sampleSize(mem.samples, n);\n\t\t};\n\treturn mem;\n}\n\nvar createModel = () => {\n\tlet model = {};\n\tmodel.init = (hiddenLayerSizes, numStates, numActions, batchSize, memSize) => {\n\t\tmodel.net = tf.sequential();\n\t\tmodel.numStates = numStates;\n\t\tmodel.numActions = numActions;\n\t\tmodel.net.add(tf.layers.dense({\n\t\t\tunits:numStates,\n\t\t\tactivation: 'tanh',\n\t\t\tinputShape: [numStates]\n\t\t}));\n\t\tmodel.net.add(tf.layers.dropout({rate: 0.2}))\n\t\thiddenLayerSizes.forEach((layerSize,i) => {\n\t\t\tmodel.net.add(tf.layers.dense({\n\t\t\t\tunits:layerSize,\n\t\t\t\tactivation: 'tanh',\n\t\t\t\tinputShape: undefined\n\t\t\t}));\t\n\t\t});\n\t\tmodel.net.add(tf.layers.dense({units:numActions}));\n\t\tmodel.net.summary();\n\t\tmodel.net.compile({optimizer:'adam', loss:'meanSquaredError'});\n\t\tmodel.memory = createMemory(memSize);\n\t};\n\tmodel.predict = (state) => {\n\t\treturn tf.tidy(()=>{return model.net.predict(state)});\n\t};\n\tmodel.nextAction = (state,eps) => {\n\t\tlet action=0;\n\t\tif (Math.random() < eps) {\n\t\t\taction = Math.floor(Math.random() * model.numActions);\n\t\t}else{\n\t\t\taction = tf.tidy(()=>{\n\t\t\t\treturn model.net.predict(state).argMax(1).dataSync()[0];\n\t\t\t}); \n\t\t}\n\t\treturn action;\n\t};\n\tmodel.train = () => {\n\t\n\t};\n\treturn model;\n};\n\n/////////////////////////////////////////////////MODEL INIT\nvar model = createModel();\n\nmodel.init(structure, beatCount + sourceMemory + agentSTMSize, 2, 4, replayMemory);\n\n\nvar createEnvironment = (beatCount, agentSTMSize, sourceMem) => {\n\tlet env = [];\n\tenv.timeEncodingSize = beatCount;\n\tenv.beatCount = beatCount;\n\tenv.agentSTMSize = agentSTMSize;\n\tenv.sourceMemSize = sourceMem;\n\tenv.stateSize = env.timeEncodingSize + env.sourceMemSize + env.agentSTMSize;\n\tenv.createInitState = () => {\n\t\t//state = memory of the environment ++ memory of past actions\n\t\tlet newState = new Array(env.stateSize).fill(-1);\n\t\treturn newState; \n\t}\n\tenv.update = (currState, beatTime, input, action) =>{\n\t\tlet newState = [];\n\t\t//time encoding\n\t\tfor(let i=0; i < env.beatCount; i++) {\n\t\t\tnewState.push(i==beatTime ? 1 : -1);\n\t\t}\n\t\t//input history\n\t\t//shift agent memory\n\t\tfor(let i=env.beatCount+1; i < env.beatCount + env.sourceMemSize; i++) {\n\t\t\tnewState[i] = currState[i-1];\n\t\t}\n\t\tnewState[env.beatCount] = input;\n\t\t\n\t\t//agent STM\n\t\tif (env.agentSTMSize > 0) {\n\t\t\tfor(let i=(env.beatCount + env.sourceMemSize)+1; i < env.stateSize; i++) {\n\t\t\t\tnewState[i] = currState[i-1];\n\t\t\t}\n\t\t\tnewState[env.beatCount + env.sourceMemSize] = action ? 1 : -1;\n\t\t}\n\t\treturn newState;\n\t}\n\treturn env;\n}\n\n\nvar env = createEnvironment(beatCount, agentSTMSize, sourceMemory);\n\nvar state = env.createInitState();\n\nvar lastAction=0;\n\nvar step = async (currState, beatNum, input, expectedoutput, sampleSize, eps, learning) => {\n\t\n\tlet action, reward, newState;\n\tif (learning) {\n\t\taction =model.nextAction(tf.tensor2d(currState,[1,currState.length]),eps);\n\t\t//console.log(\"exp: \", expectedoutput, \", action: \", action);\n\t\treward = expectedoutput == action ? 1 : -1;\n\t}else{\n\t\taction=lastAction;\n\t}\n\t\n\tnewState = env.update(currState, beatNum, input, action);\n\tif (debug) {\n\t\tconsole.log(\"Curr: \", currState, \"New\", newState, \"Action: \", action, \"Reward: \", reward);\n\t}\n\t\n\tif(!learning) {\n\t\taction =model.nextAction(tf.tensor2d(newState,[1,newState.length]),eps);\n\t}\n\t\n\tlastAction=action;\n\t\n\tif (learning) {\n\t\tmodel.memory.addSample([currState, reward, action, newState]);\n\n\t\t//calc rewards\n\t\tlet x=[];\n\t\tlet targets=[];\n\t\tlet memSamples = model.memory.sample(sampleSize);\n\t\tlet debug=[];\n\t\tif (memSamples.length >0) {\n\t\t\tmemSamples.forEach(\n\t\t\t\t([sampleState, sampleReward, sampleAction, sampleNextState],i) => {\n\t\t\t\t\ttf.tidy(()=>{\n\t\t\t\t\t\tlet val = model.predict(tf.tensor2d(sampleState, [1,sampleState.length])).dataSync();\n\t\t\t\t\t\tlet maxValNextState = model.predict(tf.tensor2d(sampleNextState, [1,sampleNextState.length])).max().dataSync()[0];\n\t\t\t\t\t\tval[sampleAction] = sampleReward; \n\t\t\t\t\t\t\n\t\t\t\t\t\tx.push(sampleState);\n\t\t\t\t\t\ttargets.push(val);\n//\t\t\t\t\t\tdebug.push([sampleState[0], sampleState[1], sampleAction, sampleReward, val[0], val[1]]);\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t);\n//\t\t\tconsole.table(debug);\n\t\t\t//learn\n\t\t\tlet xtf = tf.tensor2d(x, [x.length, model.numStates]);\n\t\t\tlet ytf = tf.tensor2d(targets, [targets.length, model.numActions]);\n\t\t\tfunction onBatchEnd (x, logs){\n\t\t\t};\n\t\t\tawait model.net.fit(xtf,ytf, {batchSize:32, epochs: 1, callbacks: {onBatchEnd} }).then(info => {console.log('loss', info.history.loss);});\n\t\t}\n\t}\n\treturn [newState, action, reward];\n}\n\nvar trigOut = createOutputChannel(0, 1);\nvar nextAction = 0;\nvar runningReward = 0;\n\ninput = async (id,x) => {\n\t//console.log(id,x);\n\tlet targetInput=x[1];\n\tlet inputVal = x[0] ? 1 : -1;\n\tlet beatNum = Math.floor(x[2]*beatCount);\n\tlet memSampleSize=replaySize;\n\tif (targetInput ==-1) {\n\t\t//we're in prediction mode\n\t\t[state, nextAction] = await step(state, beatNum, inputVal, 0, memSampleSize, 0.0, 0);\n\t\t\n\t\ttrigOut.send(nextAction);\n\t\tconsole.log('sent prediction',nextAction);\n\t}else{\n\t\t//we're learning\n\t\tlet expectedAction = x[1];\n\t\t[state, nextAction, r] = await step(state, beatNum, inputVal, expectedAction, memSampleSize, 0.0, 1);\n\t\trunningReward = (0.9 * runningReward) + (0.1 * r);\n\t\t//console.log(\"R\", runningReward);\n\t}\n}\n\n//references...\n//https://medium.com/@pierrerouhard/reinforcement-learning-in-the-browser-an-introduction-to-tensorflow-js-9a02b143c099\n//https://www.freecodecamp.org/news/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8/\n\n\n"
		},
		"id": "_zj92yxxn0"
	},
	{
		"2": {
			"drag": {
				"top": null,
				"left": null,
				"dragging": false
			},
			"resize": {
				"width": null,
				"height": null,
				"resizing": false
			},
			"responsive": {
				"valueW": 0
			},
			"min": {},
			"max": {},
			"x": 0,
			"y": 3,
			"w": 1,
			"h": 4
		},
		"3": {
			"drag": {
				"top": null,
				"left": null,
				"dragging": false
			},
			"resize": {
				"width": null,
				"height": null,
				"resizing": false
			},
			"responsive": {
				"valueW": 0
			},
			"min": {},
			"max": {},
			"x": 0,
			"y": 3,
			"w": 2,
			"h": 4
		},
		"6": {
			"drag": {
				"top": null,
				"left": null,
				"dragging": false
			},
			"resize": {
				"width": null,
				"height": null,
				"resizing": false
			},
			"responsive": {
				"valueW": 0
			},
			"min": {},
			"max": {},
			"x": 0,
			"y": 3,
			"w": 4,
			"h": 4
		},
		"8": {
			"drag": {
				"top": null,
				"left": null,
				"dragging": false
			},
			"resize": {
				"width": null,
				"height": null,
				"resizing": false
			},
			"responsive": {
				"valueW": 0
			},
			"min": {},
			"max": {},
			"x": 0,
			"y": 3,
			"w": 6,
			"h": 4
		},
		"12": {
			"drag": {
				"top": null,
				"left": null,
				"dragging": false
			},
			"resize": {
				"width": null,
				"height": null,
				"resizing": false
			},
			"responsive": {
				"valueW": 0
			},
			"min": {},
			"max": {},
			"x": 0,
			"y": 3,
			"w": 10,
			"h": 4
		},
		"data": {
			"name": "hello world_1z19h81ii_zj92yxxn0",
			"type": "modelEditor",
			"hasFocus": false,
			"theme": "shadowfox",
			"background": "rgba(25, 25, 25, 0.3)",
			"id": "_zj92yxwn0",
			"content": "importScripts(\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js\");\n____\n// STEP 1: CONFIGURE THE VARIABLES BELOW\n// STEP 2: RUN THIS SCRIPT\n// STEP 3: START LIVECODING\n// .. and repeat\n\n// the agent keeps a history of its past inputs and corresponding actions. When training, it samples from this history.  If you want the agent to learn a wide range of behaviours, make this larger, although the agent may take longer to pick up new behaviours\nvar replayMemory = 128;\n\n//when the agent trains, this is the number of past events it learns from (randomly chosen from the replay memory)\nvar replaySize = 32;\n\n//this must match up with the :beatcount: variable in LC.  This is used to configure the model so that it knows the number of the current beat within a bar\nvar beatCount=8;\n\n//the agent must listen to the source sequence, and remember the past X events (this must be at least 1).  If the number is large, the agent may be slow to respond to changes in the source sequence\nvar sourceMemory=1;\n\n//how many of the agents own past actions will it remember and base its predictions on? (0 or above).  If the number is large, the agent may be slow to respond to changes in the source sequence.  If it's larger than sourceMemory, the agent may ignore external events in preference to its own history\nvar agentSTMSize = 0;\n\n//layer sizes of the neural network\nvar structure = [32,16,8];\n\n//display state information\nvar debug=0;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimportScripts('https://cdnjs.cloudflare.com/ajax/libs/lodash.js/4.17.15/lodash.js');\ntf.setBackend('cpu');\n\nvar createMemory = (maxMem) => {\n\tlet mem = {}\n\tmem.samples=[];\n\tmem.maxMemory= maxMem;\n\tmem.test= () => console.log(mem);\n\tmem.addSample = (sample) => {\n\t\t\tmem.samples.push(sample);\n\t\t\tif (mem.samples.length > mem.maxMemory) {mem.samples.shift()}\n\t\t};\n\tmem.sample = (n) => {\n\t\t\treturn _.sampleSize(mem.samples, n);\n\t\t};\n\treturn mem;\n}\n\nvar createModel = () => {\n\tlet model = {};\n\tmodel.init = (hiddenLayerSizes, numStates, numActions, batchSize, memSize) => {\n\t\tmodel.net = tf.sequential();\n\t\tmodel.numStates = numStates;\n\t\tmodel.numActions = numActions;\n\t\tmodel.net.add(tf.layers.dense({\n\t\t\tunits:numStates,\n\t\t\tactivation: 'tanh',\n\t\t\tinputShape: [numStates]\n\t\t}));\n\t\tmodel.net.add(tf.layers.dropout({rate: 0.2}))\n\t\thiddenLayerSizes.forEach((layerSize,i) => {\n\t\t\tmodel.net.add(tf.layers.dense({\n\t\t\t\tunits:layerSize,\n\t\t\t\tactivation: 'tanh',\n\t\t\t\tinputShape: undefined\n\t\t\t}));\t\n\t\t});\n\t\tmodel.net.add(tf.layers.dense({units:numActions}));\n\t\tmodel.net.summary();\n\t\tmodel.net.compile({optimizer:'adam', loss:'meanSquaredError'});\n\t\tmodel.memory = createMemory(memSize);\n\t};\n\tmodel.predict = (state) => {\n\t\treturn tf.tidy(()=>{return model.net.predict(state)});\n\t};\n\tmodel.nextAction = (state,eps) => {\n\t\tlet action=0;\n\t\tif (Math.random() < eps) {\n\t\t\taction = Math.floor(Math.random() * model.numActions);\n\t\t}else{\n\t\t\taction = tf.tidy(()=>{\n\t\t\t\treturn model.net.predict(state).argMax(1).dataSync()[0];\n\t\t\t}); \n\t\t}\n\t\treturn action;\n\t};\n\tmodel.train = () => {\n\t\n\t};\n\treturn model;\n};\n\n/////////////////////////////////////////////////MODEL INIT\nvar model = createModel();\n\nmodel.init(structure, beatCount + sourceMemory + agentSTMSize, 2, 4, replayMemory);\n\n\nvar createEnvironment = (beatCount, agentSTMSize, sourceMem) => {\n\tlet env = [];\n\tenv.timeEncodingSize = beatCount;\n\tenv.beatCount = beatCount;\n\tenv.agentSTMSize = agentSTMSize;\n\tenv.sourceMemSize = sourceMem;\n\tenv.stateSize = env.timeEncodingSize + env.sourceMemSize + env.agentSTMSize;\n\tenv.createInitState = () => {\n\t\t//state = memory of the environment ++ memory of past actions\n\t\tlet newState = new Array(env.stateSize).fill(-1);\n\t\treturn newState; \n\t}\n\tenv.update = (currState, beatTime, input, action) =>{\n\t\tlet newState = [];\n\t\t//time encoding\n\t\tfor(let i=0; i < env.beatCount; i++) {\n\t\t\tnewState.push(i==beatTime ? 1 : -1);\n\t\t}\n\t\t//input history\n\t\t//shift agent memory\n\t\tfor(let i=env.beatCount+1; i < env.beatCount + env.sourceMemSize; i++) {\n\t\t\tnewState[i] = currState[i-1];\n\t\t}\n\t\tnewState[env.beatCount] = input;\n\t\t\n\t\t//agent STM\n\t\tif (env.agentSTMSize > 0) {\n\t\t\tfor(let i=(env.beatCount + env.sourceMemSize)+1; i < env.stateSize; i++) {\n\t\t\t\tnewState[i] = currState[i-1];\n\t\t\t}\n\t\t\tnewState[env.beatCount + env.sourceMemSize] = action ? 1 : -1;\n\t\t}\n\t\treturn newState;\n\t}\n\treturn env;\n}\n\n\nvar env = createEnvironment(beatCount, agentSTMSize, sourceMemory);\n\nvar state = env.createInitState();\n\nvar lastAction=0;\n\nvar step = async (currState, beatNum, input, expectedoutput, sampleSize, eps, learning) => {\n\t\n\tlet action, reward, newState;\n\tif (learning) {\n\t\taction =model.nextAction(tf.tensor2d(currState,[1,currState.length]),eps);\n\t\t//console.log(\"exp: \", expectedoutput, \", action: \", action);\n\t\treward = expectedoutput == action ? 1 : -1;\n\t}else{\n\t\taction=lastAction;\n\t}\n\t\n\tnewState = env.update(currState, beatNum, input, action);\n\tif (debug) {\n\t\tconsole.log(\"Curr: \", currState, \"New\", newState, \"Action: \", action, \"Reward: \", reward);\n\t}\n\t\n\tif(!learning) {\n\t\taction =model.nextAction(tf.tensor2d(newState,[1,newState.length]),eps);\n\t}\n\t\n\tlastAction=action;\n\t\n\tif (learning) {\n\t\tmodel.memory.addSample([currState, reward, action, newState]);\n\n\t\t//calc rewards\n\t\tlet x=[];\n\t\tlet targets=[];\n\t\tlet memSamples = model.memory.sample(sampleSize);\n\t\tlet debug=[];\n\t\tif (memSamples.length >0) {\n\t\t\tmemSamples.forEach(\n\t\t\t\t([sampleState, sampleReward, sampleAction, sampleNextState],i) => {\n\t\t\t\t\ttf.tidy(()=>{\n\t\t\t\t\t\tlet val = model.predict(tf.tensor2d(sampleState, [1,sampleState.length])).dataSync();\n\t\t\t\t\t\tlet maxValNextState = model.predict(tf.tensor2d(sampleNextState, [1,sampleNextState.length])).max().dataSync()[0];\n\t\t\t\t\t\tval[sampleAction] = sampleReward; \n\t\t\t\t\t\t\n\t\t\t\t\t\tx.push(sampleState);\n\t\t\t\t\t\ttargets.push(val);\n//\t\t\t\t\t\tdebug.push([sampleState[0], sampleState[1], sampleAction, sampleReward, val[0], val[1]]);\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t);\n//\t\t\tconsole.table(debug);\n\t\t\t//learn\n\t\t\tlet xtf = tf.tensor2d(x, [x.length, model.numStates]);\n\t\t\tlet ytf = tf.tensor2d(targets, [targets.length, model.numActions]);\n\t\t\tfunction onBatchEnd (x, logs){\n\t\t\t};\n\t\t\tawait model.net.fit(xtf,ytf, {batchSize:32, epochs: 1, callbacks: {onBatchEnd} }).then(info => {console.log('loss', info.history.loss);});\n\t\t}\n\t}\n\treturn [newState, action, reward];\n}\n\nvar trigOut = createOutputChannel(0, 1);\nvar nextAction = 0;\nvar runningReward = 0;\n\ninput = async (id,x) => {\n\t//console.log(id,x);\n\tlet targetInput=x[1];\n\tlet inputVal = x[0] ? 1 : -1;\n\tlet beatNum = Math.floor(x[2]*beatCount);\n\tlet memSampleSize=replaySize;\n\tif (targetInput ==-1) {\n\t\t//we're in prediction mode\n\t\t[state, nextAction] = await step(state, beatNum, inputVal, 0, memSampleSize, 0.0, 0);\n\t\t\n\t\ttrigOut.send(nextAction);\n\t\tconsole.log('sent prediction',nextAction);\n\t}else{\n\t\t//we're learning\n\t\tlet expectedAction = x[1];\n\t\t[state, nextAction, r] = await step(state, beatNum, inputVal, expectedAction, memSampleSize, 0.0, 1);\n\t\trunningReward = (0.9 * runningReward) + (0.1 * r);\n\t\t//console.log(\"R\", runningReward);\n\t}\n}\n\n//references...\n//https://medium.com/@pierrerouhard/reinforcement-learning-in-the-browser-an-introduction-to-tensorflow-js-9a02b143c099\n//https://www.freecodecamp.org/news/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8/\n\n\n"
		},
		"id": "_zj92yxwn0"
	}
]